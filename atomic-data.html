<!DOCTYPE html>
<HTML>

<HEAD>

  <title>atomic_data: A Multibyte General Purpose Lock-Free Data Structure - Alexandr Poltavsky, software developer</title>

  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta http-equiv="Cache-Control" content="no-store" />

  <link rel="stylesheet" type="text/css" href="css/common.css" />
  <link rel="stylesheet" href="highlight/styles/hybrid.css">

</HEAD>

<BODY> 

<div id="wrap">

  <!-- header part -->

  <div id="header">

  <a href="/color-throne.html"><img class="ad" src="images/color-throne-logo-promo.png" title="Color Throne Game"/></a>    

  <a href="/">
      <img class="avatar" width="160" src="images/alexandr-poltavsky-avatar.jpeg" title="Alexandr Poltavsky" align="left"/>
    </a>
    <strong>
      Alexandr Poltavsky <br/> 
      Software developer <br/>
      <em>poltavsky.alexandr&#64;gmail.com</em> 
    </strong>
    <ul id="menu">
      <li><a href="/">Blog Contents</a></li>
      <li><a href="https://github.com/alexpolt/">Github</a></li>
      <li><a href="https://www.shadertoy.com/user/alexpolt">Shadertoy</a></li>
      <li><a href="https://twitter.com/poltavsky_alex">Twitter</a></li>
    </ul>

  </div>


  <div id="content" style="clear: left">

  <!-- here goes the main content -->

  <ul id="main-menu">
    <li id="main-menu-0"><a href="index.html">Programming</a></li>
    <li id="main-menu-1"><a href="index-gfx.html">Graphics</a></li>
    <li id="main-menu-2"><a href="index-off.html">Off-topic</a></li>
  </ul>


<h2>atomic_data: A Multibyte General Purpose Lock-Free Data Structure</h2>

<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#cas">Compare-And-Swap</a></li>
<li><a href="#llsc">Load Linked/Store Conditional</a></li>
<li><a href="#aba">Evil Number One: the ABA</a></li>
<li><a href="#lifetime">Evil Number Two: Lifetime Management</a></li>
<li><a href="#atomic_data">atomic_data</a></li>
<li><a href="#memory_ordering">Memory Ordering Considerations</a></li>
<li><a href="#usage">Usage (API)</a></li>
<li><a href="#issues">Exception safety, limitations and other issues</a></li>
<li><a href="#code">Code Samples</a>
<ul>
<li><a href="#atomic_data_test">Increment/Decrements of an Array</a></li>
<li><a href="#atomic_map">Lock-Free std::map?</a></li>
<li><a href="#vector_of_atomic">atomic_data as a Container Element</a></li>
<li><a href="#atomic_list">Concurrent Singly Linked List with Arbitrary Access</a></li>
</ul></li>
<li><a href="#performance">Performance Tests</a></li>
</ul>

<p><a name="intro"></a></p>

<p>Perhaps every article on lock-free programming should start with a warning that it's extremely
  hard to do right. There are certain subtle effects that lead to hard-to-catch bugs. Also, in
  an attempt to deal with it lock-free data structures and algorithms quickly become monstrous
  and entangled. Is it possible to keep everything simple and avoid bugs at the same time? We'll
  talk about it.</p>

<p>This article discusses two major problems in the lock-free Universe: the lifetime management 
  problem and the root cause of the ABA. Also we will describe a relatively new lock-free 
  data structure <strong>atomic_data</strong>. It solves the above two problems and offers a general enough 
  approach to serve as a foundation for robust lock-free algorithms. In short, you wrap your
  data in <strong>atomic_data</strong> and by using its <em>read</em> and <em>update</em> methods you get transactional
  semantics of operation under any number of threads. You can have a megabyte array for example
  and it will still work. All of the code with a Visual Studio and Android Studio projects and
  some samples is available on <a href="https://github.com/alexpolt/atomic_data">GitHub</a>.</p>

<p>As a reminder, there are three levels of guarantees: <strong>wait-free, lock-free and obstruction
  free</strong>. Wait-free is the strongest: operations on shared data basically never fail. This could 
  be achieved, for example, when reading and updating is a single atomic operation. Lock-free 
  assumes that some operations may fail, but progress is guaranteed in a fixed number of steps. 
  And finally obstruction free means progress under no contention from other threads.</p>

<p>There are two major primitives/ways for doing lock-free programming:  <strong>CAS</strong> (compare and swap) 
  and  <strong>LL/SC</strong> (load-linked/store-conditional. These are RMW ( Read-Modify-Write ) operations, 
  but they are not equivalent. Also important is the fact that these operations acquire additional 
  semantics when operating on pointers.</p>

<p><a name="cas"></a></p>

<h3>CAS (Compare-And-Swap)</h3>

<p>CAS is an indivisible operation (<em>[lock] cmpxchg</em> instruction on x86) that compares an expected
  value at a memory address and replaces it with a desired value, on fail it returns the current 
  value. Using CAS you could implement things like an atomic increment and basically any other
  atomic arithmetic operation. </p>

<p>But it gains additional meaning when used on pointers. Using CAS on a pointer could also be 
  viewed as a rename with a check. Actually <em>rename</em> is the way you do atomic file modifications 
  in Linux (rename system call is atomic if on the same device). Also important is that it 
  establishes an equivalence relationship between the pointer and the data it points to. In the 
  section on ABA we'll see how it plays out. </p>

<p>CAS provides the lock-free level of guarantee.</p>

<p><a name="llsc"></a></p>

<h3>LL/SC (Load Linked/Store Conditional).</h3>

<p>LL/SC operates with the help of a <em>link register</em> (one per core). By doing a special read 
  (lwarx on PowerPC, ldrex on ARM) the link register is initialized. Any processing is then 
  allowed and finished with a conditional store (stwcx on PowerPC, strex on ARM). </p>

<p>LL/SC is orthogonal to CAS because it's not comparing anything. It is also more general and 
  robust. But it requires more work from the OS developers: on context  switch a dummy conditional 
  store is necessary to clear the reservation. This also could manifest itself under heavy 
  contention: LL/SC could theoretically become obstruction free (CAS is always lock-free because 
  at least one thread will always succeed).</p>

<p><strong>Now to the evils that hunt lock-free programming.</strong> Here is a picture of a stack and two
  threads doing work.</p>

<p><center><img src="images/The-ABA-Problem.png" alt="" title="The dreaded ABA" /></center></p>

<p><a name="aba"></a></p>

<h3>Evil Number One: the ABA</h3>

<p>As we observed above, CAS on pointers ties together the pointer and the data it points to.
  ABA happens when our idea about this relationship no longer holds true: the state of shared 
  data has changed (broken invariant) and we are unable to detect that with the CAS (LL/LC is 
  immune to this problem) on the pointer, because that pointer might have been deleted and 
  reallocated again. The ABA results in lost data and invalid data structure.</p>

<p>ABA can be solved in a number of ways: GC, hazard pointers, versioned pointers. Another solution
  is to make the data immutable in a strict sense. I mean, not only the data itself is constant,
  but also the address it occupies is unique. This is the way a versioned pointer works: we add
  a version tag to the pointer and this has the effect of increasing the address space.</p>

<p>It really helps to think about lock-free operations in the following way: as soon as we touched 
  any piece of shared data - we initiated a transaction, and the transaction should behave
  as stated by the <a href="https://en.wikipedia.org/wiki/ACID" title="Atomicity, Consistency, Isolation, Durability">ACID</a> rules.</p>

<p>As is written above, LL/SC doesn't suffer from ABA because on context switch the reservation is
  cleared.</p>

<p><a name="lifetime"></a></p>

<h3>Evil Number Two: Lifetime Management</h3>

<p><center><img src="images/atomic-cat.png" alt="" title="Schroedinger's Cat" /></center></p>

<p>In the above image the cat is an object. The object lifetime problem in multithreaded programming 
  is fundamental because at any point in time shared data could be accessed by a thread. Using
  exclusive locking is going to work but kills performance. Also GC, hazard pointers or using
  shared_ptrs can help, but they all are not ideal. GC needs a stop-the-world pause, hazard
  pointers require checking all threads, reference counting is a performance hit and might break
  out of control.</p>

<p>In no way this is the exhaustive description of lock-free programming issues, but these two
  are the most important. For linked data structures there is also a delete problem and we'll
  cover it later.</p>

<p><strong>Now it is time for me to introduce <em>atomic_data</em> that is a good compromise between having a
  lock-free data structure and avoiding the ABA and lifetime issues.</strong></p>

<p><a name="atomic_data"></a></p>

<h3><strong>atomic_data</strong>: A Multibyte General Purpose Lock-Free Data Structure</h3>

<p><strong>atomic_data</strong> is a variant of RCU (Read-Copy-Update). Actually, at first I didn't know that 
  fact, but then, while exploring, I came upon this <a href="http://www.rdrop.com/~paulmck/RCU/" title="A dissertation about Read-Copy-Update">page</a> by  Paul McKenney. Also you can 
  read about how the RCU is used in the <a href="https://lwn.net/Articles/262464/" title="What is RCU, Fundamentally?">Linux Kernel</a>. After studying different 
  implementations I came to a conclusion that  <strong>atomic_data</strong> has a novel design and  therefore 
  has some value for the community. But if you feel like that was already done before - feel free 
  to leave a comment.</p>

<p>The one important aspect that divides RCU techniques is in how reclamation of used memory is
  done. We need a grace period - when data is no longer accessed by threads - to do cleaning.
  This way we solve the lifetime management problem but not necessarily the ABA problem. </p>

<p><strong>atomic_data</strong> is a template that wraps any data structure. There is a static preallocated 
  circular lock-free queue of this data of desired length. C++ rules for templates and static 
  data makes it one instance per data type. The following illustration will make it easier to 
  understand.</p>

<p><center><img src="images/atomic-data.png" alt="" title="A diagramm of atomic_data operation" /></center></p>

<p>During operation the <em>update</em> and <em>read</em> methods are being called that accept a functor (a lambda)
  as a parameter provided by the user. It atomically allocates an element from the circular 
  lock-free queue, makes a copy of the current data and passes it to the functor. After the functor 
  finishes, the update method tries to do CAS on the pointer to current data. In case of a failure 
  the steps are repeated. On success the now old data element is being atomically returned to the 
  circular queue. There is also an <em>update_weak</em> method that doesn't loop and returns a boolean.</p>

<p>So how do we avoid reusing used data and the ABA problem? That's where <strong>atomic_data</strong> differs:
  we use a backing multi-producer/multi-consumer queue and introduce <em>a synchronization barrier</em>.
  This sync barrier creates a "time out" during which <strong>atomic_data</strong> waits for old reads and
  updates to complete, no threads are touching data. But does it mean that readers are also
  blocked from reading data during that period? Here is a nice hack: the backing array for 
  the queue is double the size that makes possible to use two atomic counters to watch for the data 
  usage. This allows reading to be independent of the sync barrier and be wait-free (read the code 
  for more details). So reading is never blocked.</p>

<p>When I got the idea for the design of <strong>atomic_data</strong> I had to choose how to implement a 
  lock-free multi-producer/multi-consumer queue. I didn't suspect that doing this right is 
  extremely difficult! I ran into every possible concurrency bug. There were three candidates in 
  the end: two MP/SC queues with a single-threaded broker in between, a MP/MC queue requiring DCAS 
  (Double CAS) to fight overrunning and finally a MP/MC queue with a sync barrier (I think I need 
  to blog about it in a different post). After testing the best implementation was the queue with
  a barrier: it had a simple design, it worked very reliably, it didn't require DCAS and finally
  it showed good performance.</p>

<p><a name="memory_ordering"></a></p>

<h3>Memory Ordering Considerations</h3>

<p>The cores of most modern CPUs are <a href="https://en.wikipedia.org/wiki/Out-of-order_execution" title="Out-of-order execution">out-of-order</a> (multi-core systems are out-of-order by
  definition). That means we have to consider the model of execution of our code. When it comes
  to memory we are dealing with a <a href="https://en.wikipedia.org/wiki/Memory_model_(programming)" title="Memory model">memory model</a>. Another form of out-of-order execution is the 
  <a href="http://preshing.com/20120930/weak-vs-strong-memory-models/" title="Weak vs. Strong Memory Models">reordering of memory operations</a>. For example, Intel processors can <a href="http://preshing.com/20120515/memory-reordering-caught-in-the-act/" title="Memory Reordering Caught in the Act">reorder</a> a store 
  followed by load, but not a load followed by store. In that regard Intel CPUs are strongly-ordered. 
  On the other hand ARM cores are weakly-ordered and can reorder both store-load and load-store 
  operations.</p>

<p>Also the compilers themselves are capable of doing optimizations with reordering of instructions. 
  To deal with it on both the hardware and software level C++11 introduced a <a href="http://en.cppreference.com/w/cpp/language/memory_model" title="C++ Memory Model">C++ memory model</a>. 
  First, it has the notion of <em>a data race</em> - reading and writing the same memory location from 
  different threads. To deal with data races <strong>atomic_data</strong> uses <em>std::atomic</em> data type from C++ 
  standard library. Second, the C++ memory model introduces the concept of a <a href="http://en.cppreference.com/w/cpp/atomic/memory_order" title="C++ Memory Order">memory order</a> and
  defines them: relaxed, acquire, release and sequentially consistent. The seq-cst memory order is
  the strongest but also pessimistic in terms of possible optimizations. If you want more details,
  I really suggest you take some time and read the Jeff's blog and watch Herb Sutter's <a href="http://channel9.msdn.com/Shows/Going+Deep/Cpp-and-Beyond-2012-Herb-Sutter-atomic-Weapons-1-of-2" title="Atomic Weapons">talk</a>.</p>

<p>When we use standard synchronization primitives like mutexes then we don't usually have to
  think about memory ordering, because it's done for us implicitly. But when doing concurrent 
  programming using RMW by hand we take full responsibility enforcing correct ordering on different
  platforms. For that the methods of the <em>std::atomic</em> data type take a memory order parameter.</p>

<p>To test the correctness of <strong>atomic_data</strong> operation I used an array of some size and the task 
  was to find the minimum element and increment it. Given the right size of the array, the number 
  of threads and the number of iterations for each thread the result should be that every array 
  cell contains the same number: iterations * number of threads / array size. For example, 
  4 threads each doing 8192 increment iterations on an array of size 256: the entire array must 
  contain the number 4*8192/256 = 128 in the end.</p>

<p>For the first draft of <strong>atomic_data</strong> every atomic operation was using relaxed semantics just 
  to see what happens. Guess my surprise when on my 2 Core Intel machine the test was successful!
  What happened? Where is the memory ordering hell? This was my first revelation into strongly
  ordered CPUs. As <a href="http://preshing.com/20120515/memory-reordering-caught-in-the-act/">Jeff Preshing mentions</a>, on x86 the locked instructions act as memory 
  barriers themselves. Also using static globals for the queue didn't left much opportunity for the 
  compiler reordering of the source code. </p>

<p>But that's not the end of the story. The next in line for the test was my Samsung Galaxy S5 CPU: 
  a 4 Core ARM. I created a simple project using Android NDK (thanks Google the C++ compilers are 
  up-to-date and everything works, though some reading and googling might still be necessary ) and 
  fired it up. BAM! Several thousand increments  were missing! Nice, I was looking for this. 
  Actually, I really advise you to get the Android Studio project for <strong>atomic_data</strong> and comment 
  the two atomic_thread_fence calls there. After that fire it up on your smartphone, see the 
  missing counts and then try to place the fences in the code yourself. The puzzle is - using
  the minimum number of fences achieve correct behaviour.</p>

<p>Observing the memory operation's reordering in the act was a nice experience. Gradually I got
  the intuition for what happens and solved the problem. It took me just two release fences.
  Here I'd like to mention the following observation: if a thread reads a variable that's behind
  a release barrier, it means all the memory stores before the observed value has reached the
  memory. I mean, if your reads are dependent then you can skip having an acquire fence. Actually,
  that is what you can read about in Jeff Preshing's <a href="http://preshing.com/20120930/weak-vs-strong-memory-models/" title="Weak vs. Strong Memory Models">post</a> ( a-&gt;b ), where he also mentions
  that it is not true for Alpha CPUs.</p>

<p><center><img src="images/ordered-unorder-stores.jpg" alt="" title="Sheep are stores. A man is a memory barrier" /></center></p>

<p><a name="usage"></a></p>

<h2>Usage (API)</h2>

<p>The greatest advantage of <strong>atomic_data</strong> in comparison with other lock-free techniques is
  the easy of using it. <strong>atomic_data</strong> is a template that wraps your data structure. It can 
  be anything, the only requirement for it to be copyable and default constructable. </p>

<p>You have two basic methods to read and modify your data. The <em>read</em> method takes a functor
  that has a single parameter of a type <em>"pointer to data structure type"</em> that points to the
  latest version of the data and returns whatever the passed functor returns:</p>

<pre><code>    struct mydata {
      int a;
      float b;
      double c;
    };

    atomic_data&lt; mydata &gt; mydata0;

    //you can optionally add a second template parameter that sets the backing queue size
    //making it equal 2 * number of threads is usually a good guess

    //reading and returning field a

    auto result = mydata0.read( []( mydata* data ) {

      return data-&gt;a;
    } );
</code></pre>

<p>The same thing is for the <em>update</em>, except it takes <em>a pointer to a copy of the current data</em>
  and returns a <em>bool</em> indicating whether you want for the update to happen or not. <strong>Data is 
  updated atomically.</strong></p>

<pre><code>    //atomically updating the data

    mydata0.update( []( mydata* data ) {

      data-&gt;b = data-&gt;b + 1.0f;
      data-&gt;a++;

      return true;
    } );
</code></pre>

<p>You can read the <a href="https://github.com/alexpolt/atomic_data/blob/master/atomic_data.h" title="atomic_data header file">source code</a> of <strong>atomic_data</strong> on GitHub. It is a single header file 
  and is just 300 lines of code. The simplicity of its implementation is another great plus.</p>

<p><a name="issues"></a></p>

<h3>Exception safety, limitations and other issues</h3>

<p>The <em>read</em> and <em>update</em> methods - the workhorses - are exception safe. They employ RAII 
  techniques to maintain invariants. There is also static initialization which calls new to
  fill the queue and destructor of <strong>atomic_data</strong> calls delete. Nothing is done there to
  guard against exceptions.</p>

<p>Special attention should be paid to the fact that <strong>atomic_data::update</strong> method is not
  reentrant, because you might hit a barrier and a second call to update will spin forever.
  <strong>atomic_data::update_weak</strong> on the other hand is reentrant. Though you should keep in 
  mind that in no way two recursive calls to this method creates a transaction. Those are
  going to be separate updates.</p>

<p>You might rightfully ask if <strong>atomic_data</strong> is really lock-free since it has a sync barrier.
  Actually, if you think about it for a moment, due to limited computer memory some synchronization 
  is unavoidable no matter what lock-free technique you use. Whether it be a GC pause or a
  check of all other threads or reference counting or something else. Reference counting and
  LL/LC to get around ABA might seem like a perfect match, but reference counting brings
  its own problems to the table. <strong>atomic_data</strong> is a good compromise and its flexible because
  you can adjust the length of the queue. Also a sync barrier helps uncover bugs earlier.</p>

<p>By the way what the length of the backing queue should be. From my tests it seems like 2x 
  number of threads is quite enough. Actually <strong>atomic_data</strong> works even with the queue of 
  length 1 (which I was surprised to find out). Nothing prevents from making the queue
  longer and offset the cost of the synchronization events.</p>

<p>Another often cited feature of <em>true</em> lock-free data structures is the tolerance to thread
  killings or suspensions. I am a bit confused by this argument. After all we are creating
  programs under the assumption of documented behaviour from the OS and hardware. We rely on
  fair scheduling policies from the OS and no spurious kill signals or suspensions should
  happen. And threads are not inferior to processes in any way. The only real issue is 
  preemption and the picture below provides a Concurrency Visualizer graph of thread scheduling.</p>

<p><center><img src="images/atomic-data-trace.png" alt="" title="The effect of preemption" /></center></p>

<p><a href="images/atomic-data-trace-orig.png" title="Clean version of above">Here is the pic</a> without drawing.</p>

<p>If it becomes a real problem in your program then you can increase the size of the queue.
  But from my tests it was never an issue and performance results speak for themselves (some
  numbers are at the very end).</p>

<p><a name="code"></a></p>

<h2>Code Samples</h2>

<p><a name="incdec"></a></p>

<h3>Increments/Decrements of an Array</h3>

<p>Now comes my favorite part because the design of <strong>atomic_data</strong> allows to create really
  cool things. Let us start with a test that was already described above (Memory Ordering part):
  a number of threads look up the minimum value in an array of some size and increment it. 
  By the end of execution we expect all array cells to contains some know number.</p>

<pre><code>    const size_t array_size = 64;
    const unsigned threads_size = 8;
    const unsigned iterations = 81920;

    //here comes the array, we can use any size, atomic\_data's read and update 
    //methods make the behaviour transactional
    struct array_test {
        unsigned data[ array_size ];
    };

    //an instance of atomic_data, queue is 2x threads_size
    atomic_data&lt;array_test, 2*threads_size&gt; atomic_array;

    //called by each thread
    void update_array() {

      //update, we are given a copy that we should update (RCU)
      atomic_array.update( []( array_test* array_new ) {

        unsigned min = -1;
        size_t min_index = 0;

        //look up the minimum value
        for( size_t i = 0; i &lt; array_size; i++ ) {
            if( array_new-&gt;data[i] &lt; min ) {
              min = array_new-&gt;data[i];
              min_index = i;
            }
        }

        array_new-&gt;data[ min_index ]++;

        //tell update that we are good to go
        return true;

      } );

    }
</code></pre>

<p>At the end of execution every array element must be equal <strong>threads_size*iterations/array_size 
  = 10240</strong>. Check out the code for this example on <a href="https://github.com/alexpolt/atomic_data/blob/master/samples/atomic_data_test.cpp">Github</a>.</p>

<p>There is also an <a href="https://github.com/alexpolt/atomic_data/tree/master/AndroidStudio/atomic_data_test/app/src/main/jni">Android Studio</a> project with this test and you can observe the 
  effects of memory ordering on the result (try removing std::atomic_thread_fences in 
  atomic_data.h) if you run it on a smartphone with a weakly ordered many-core processor (like ARM).</p>

<p>One thing to remember though is that <strong>atomic_data</strong> guards only the data it holds. If inside
  the update method the code updates some other global or captured data - that's gonna bite.</p>

<p><a name="atomic_map"></a></p>

<h3>Lock-Free std::map?</h3>

<p>The design of the <strong>atomic_data</strong> makes it really easy to turn any data structure into a 
  concurrent one. All we need is to wrap it in <strong>atomic_data</strong> and use provided <em>read</em> and 
  <em>update</em> methods to access it.</p>

<pre><code>    atomic_data&lt; std::map&lt;key, value&gt; &gt; atomic_map;

    atomic_map.update( []( auto *map_new ) {

        map_new-&gt;insert( { key, value } );

        return true;
    } );

    auto it = atomic_map.read( []( auto *map ) {

        auto it = map-&gt;find( key );

        return it;
    } );
</code></pre>

<p>Here is an <a href="https://github.com/alexpolt/atomic_data/blob/master/samples/atomic_map.cpp">example</a>. Long time ago Andrei Alexandrescu offered such a map in his 
  <a href="http://erdani.com/publications/cuj-2004-10.pdf" title="Lock-Free Data Structures. 17 December 2007.">article</a>. This one is definitely better.</p>

<p>Yes, it works. But the cost of copying makes it slower than using a mutex unless the access 
  pattern is mostly reading. Actually for an <strong>atomic_data&lt; std::vector &gt;</strong> the story is
  different: vector can skip memory allocation on copying and it makes it quite fast. 
  <a href="https://github.com/alexpolt/atomic_data/blob/master/samples/atomic_vector.cpp">Try it</a> on your machine.</p>

<p><a name="vector_of_atomic"></a></p>

<h3>atomic_data as a Container Element</h3>

<p><strong>atomic_data</strong> is copyable and movable and it can be used as a container element.</p>

<pre><code>      std::vector&lt; atomic_data&lt;int&gt; &gt; vector{ some_size };

      vector[0].update( []() { .... } );

      auto result = vector[0].read( [](){ .... } );

      std::sort( begin( vector ), end( vector ) );
</code></pre>

<p>Here is a <a href="https://github.com/alexpolt/atomic_data/blob/master/samples/vector_of_atomic.cpp">sample</a>.</p>

<p><a name="atomic_list"></a></p>

<h3>Concurrent Singly Linked List with Arbitrary Access</h3>

<p>This is where <strong>atomic_data</strong> comes to its full glory. Compared to <a href="https://www.youtube.com/watch?v=CmxkPChOcvw" title="CppCon 2014: Lock-Free Programming (or, Juggling Razor Blades), Part II">Herb Sutter's solution</a> 
  it doesn't require any special support from the std::atomic library, no need for Double CAS, 
  no ABA and you can safely store iterators to list elements and dispose of them when necessary.</p>

<p>Here is the basic structure:</p>

<pre><code>    template&lt; typename T, unsigned queue_length &gt; struct atomic_list {

      struct node;

      using atomic_node = atomic_data&lt;node&gt;;
      using node_ptr = std::shared_ptr&lt;atomic_node&gt;;

      struct node {
        bool lock;
        T0 data;
        node_ptr next;
      };

      ...

    };
</code></pre>

<p>Basically it's a list of <strong>atomic_data</strong> objects and the next pointer is wrapped in shared_ptr.</p>

<p>When we talk about lock-free linked data structures there is one particular problem they suffer
  from: <strong>the deletion problem</strong>. The drawing below describes it quite good.</p>

<p><center><img src="images/deletion-problem.png" alt="" title="The deletion problem in lock-free lists" /></center></p>

<p>Easy to get orphaned data. The easiest solution is to lock the node before deleting it. 
  <strong>atomic_data</strong> here really helps. It allows for the atomic modification of all of the members of 
  the node and makes implementation really clear and short. As a result we can write code like this:</p>

<pre><code>    //create an instance
    atomic_list&lt;int&gt; atomic_list0;

    //insert at the beginning, get iterator to the result
    auto it = atomic_list0.insert( value );

    //getting to the data under concurrency
    auto value = it-&gt;read( [](auto* data) { return data-&gt;value; }

    //removing
    for( auto it = atomic_list0.begin(); it; ++it ) {

      //*it returns a ref to atomic_data
      if( (*it)-&gt;value == search_value ) {

          //note the weak suffix, we can't be sure that an element is not locked
          auto r = auto atomic_list0.remove_weak( it );

      }

    }

    //iterating
    for( auto&amp; element : atomic_list0 ) { ... }
</code></pre>

<p>Here is an example to test the correctness: we prepopulate an instance of <strong>atomic_list</strong> with
  array_size number of elements. Then we launch threads that perform equal number of insertions 
  and deletions at random positions. At the end we expect the list to remain the initial size.
  Get a look at the code on <a href="https://github.com/alexpolt/atomic_data/blob/master/samples/atomic_list.cpp">Github</a>.</p>

<p><em>UPDATE</em>. After thinking about it more I realized that there is no escaping of reference
  counting as you move through the list (you need nodes alive), and it essentially means hand-over 
  locking. So in case you have a lean shared_ptr (to avoid DCAS) you just need to place two locks 
  on the nodes, do the job of deleting or adding (adding needs a single lock), and be very accurate
  with shared_ptrs, because you going to have a mismatch between the number of shared_ptrs and
  reference counters, basically you'll have to add a release() method to a shared_ptr. That's all
  for a concurrent singly linked list.</p>

<p><a name="performance"></a></p>

<h2>Performance</h2>

<p>So what about efficiency of <strong>atomic_data</strong>. Is it all worth learning about it? I say firm yes.
  The baseline was a bare mutex locking all reads and updates: the <a href="https://github.com/alexpolt/atomic_data/blob/master/samples/atomic_data_mutex.h">same interface</a> as with 
  <strong>atomic_data</strong> but no copying, no queues, just simple locking.</p>

<p>There are three basic cases: updates and zero reads, equal number of update and read calls, and
  reads prevailing over updates. Also important is the size of guarded data.</p>

<p><TABLE>
  <TR><TH>OS
      <TH colspan="4">Windows 7<br>2 Core AMD<br>VS2015
      <TH colspan="4">Android 5<br>4 Core ARM<br>NDK r12b, gnustl
      <!--TH colspan="4">Ubuntu <br>2 Core Intel-->
  <TR><TH>data size <br />
      <TH>1   <TH>8   <TH>64    <TH>256 
      <TH>1   <TH>8   <TH>64    <TH>256
      <!--TH>1   <TH>8   <TH>64    <TH>256-->
  <TR><TH>many updates<BR>zero reads <br />
      <TD class="red">110<BR>220  <TD class="red">160<BR>290 <TD class="red">300<BR>500 <TD class="red">750<BR>1000
      <TD class="green">570<BR>150  <TD class="green">380<BR>150 <TD class="green">1000<BR>250 <TD class="green">1600<BR>480
      <!--TD><BR>  <TD><BR> <TD><BR> <TD><BR-->
  <TR><TH>many updates<BR>many reads <br />
      <TD class="red">130<BR>250  <TD class="red">210<BR>290 <TD class="green">580<BR>510 <TD class="green">1400<BR>1200
      <TD class="green">330<BR>150  <TD class="green">890<BR>170 <TD class="green">1400<BR>310 <TD class="green">2400<BR>700
      <!--TD><BR>  <TD><BR> <TD><BR> <TD><BR-->
 <TR><TH>many updates<BR>20x more reads 
      <TD class="red">280<BR>300  <TD class="green">690<BR>620 <TD class="green">3500<BR>2000 <TD class="green">13400<BR>6800
      <TD class="green">720<BR>165  <TD class="green">1750<BR>215 <TD class="green">7000<BR>800 <TD class="green">25000<BR>2700
      <!--TD><BR>  <TD><BR> <TD><BR> <TD><BR-->
 <TR><TH colspan="9"><span style="color: #500000">red</span> background - mutex wins, 
                     <span style="color: #005000">green</span> background - atomic_data wins
</TABLE></p>

<p>To get the numbers I used the array increments/decrements sample - the first in the Samples 
  section above. These are milliseconds but they are relative just to compare the performance with 
  the baseline std::mutex case. The first line is for the mutex, the second is <strong>atomic_data</strong>.
  The queue size for the <strong>atomic_data</strong> was 2*threads_size and threads_size = 8.</p>

<p>You can see how good the std::mutex on Windows 7 is (it was compiled with Visual Studio 2015). 
  Looking up on the Net it seems the implementation busy spins for a while (optimistic logic) and 
  the fact, that increasing the data size changes the situation, supports that guess.</p>

<p>On Android the timings are especially sweet. The only problem was that Android dynamically
  adjusts priorities and the mutex version had quite different timings (but always worse that
  <strong>atomic_data</strong>).</p>

<p>The results show that <strong>atomic_data</strong> is quite competitive even with it copying the data on 
  every update. But we need more testing and I ask for your help here. Clone or download the 
  <a href="https://github.com/alexpolt/atomic_data">GitHub</a> repo and run the tests on your machine. <strong>atomic_data</strong> works both on 32-bit 
  and 64-bit machines.</p>

<p>You're a hero if you've read all of the article.</p>


<script src="highlight/highlight.pack.js"></script>
<script>
  //hljs.configure({languages: ["C++"]});
  hljs.initHighlightingOnLoad();
</script>


<div id="disqus_comments"><a href="document.getElementById('disqus').src='js/disqus.js';">read and write commentaries</a></div>
<div id="disqus_thread"></div>
<script id="disqus">
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    //if( ! document.getElementById("contents") ) // alexpolt: exclude main page with contents
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        
        s.src = '//alexpolt-github-io.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



  </div> <!-- end content -->

</div> <!-- end wrap -->



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75341409-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript">
    (function (d, w, c) {
        (w[c] = w[c] || []).push(function() {
            try {
                w.yaCounter43425229 = new Ya.Metrika({
                    id:43425229,
                    clickmap:true,
                    trackLinks:true,
                    accurateTrackBounce:true
                });
            } catch(e) { }
        });

        var n = d.getElementsByTagName("script")[0],
            s = d.createElement("script"),
            f = function () { n.parentNode.insertBefore(s, n); };
        s.type = "text/javascript";
        s.async = true;
        s.src = "https://mc.yandex.ru/metrika/watch.js";

        if (w.opera == "[object Opera]") {
            d.addEventListener("DOMContentLoaded", f, false);
        } else { f(); }
    })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/43425229" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
<!-- /Yandex.Metrika counter -->

</BODY>

