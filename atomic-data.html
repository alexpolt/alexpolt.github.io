<!DOCTYPE html>
<HTML>

<HEAD>

  <title>Alexandr Poltavsky, software developer</title>

  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta http-equiv="Cache-Control" content="no-store" />

  <link rel="stylesheet" type="text/css" href="css/common.css" />
  <link rel="stylesheet" href="highlight/styles/hybrid.css">

</HEAD>

<BODY> 

<div id="wrap">

  <!-- header part -->

  <div id="header">

    <a href="/energia-buran.html">
      <img width="160" src="images/alexandr-poltavsky-avatar.jpeg" title="Energia-Buran Military Space System" align="left"/>
    </a>
    <strong>
      Personal blog of Alexandr Poltavsky. <br/> 
      Software developer. <em>alexpolt&#64;yandex.ru</em><br/>
    </strong>
    <ul id="menu">
      <li><a href="/">Contents</a></li>
      <li><a href="https://github.com/alexpolt/">Github</a></li>
    </ul>

  </div>


  <div id="content" style="clear: left">

  <!-- here goes the main content -->

<h2><strong>atomic_data</strong>: A Multibyte General Purpose Lock-Free Data Structure</h2>

<p>Perhaps every article on lock-free programming should start with a warning that it's extremely
  hard to do right. There are certain subtle effects that lead to hard-to-catch bugs. Also, in
  an attempt to deal with it lock-free data structures and algorithms quickly become monstrous
  and entangled. Is it possible to keep everything simple and avoid bugs at the same time? We'll
  talk about it.</p>

<p>This article discusses two major problems in the lock-free Universe: the lifetime management 
  problem and the root cause of the ABA. Also we will describe a relatively new lock-free 
  data structure <strong>atomic_data</strong>. It solves the above two problems and offers a general enough 
  approach to serve as a foundation for robust lock-free algorithms. </p>

<p>As a reminder, there are three levels of guarantees: <strong>wait-free, lock-free and obstruction
  free</strong>. Wait-free is the strongest: operations on shared data basically never fail. This could 
  be achieved, for example, when reading and updating is a single atomic operation. Lock-free 
  assumes that some operations may fail, but progress is guaranteed in a fixed number of steps. 
  And finally obstruction free means progress under no contention from other threads.</p>

<p>There are two major primitives/ways for doing lock-free programming:  <strong>CAS</strong> (compare and swap) 
  and  <strong>LL/SC</strong> (load-linked/store-conditional. These are RMW ( Read-Modify-Write ) operations, 
  but they are not equivalent. Also important is the fact that these operations acquire additional 
  semantics when operating on pointers.</p>

<p>A one of kind resource on the Internet about multithreaded programming, memory barriers,
  lock-free techniques, etc. is the website by <a href="http://preshing.com/about/">Jeff Preshing</a>. </p>

<h3>CAS (Compare-And-Swap)</h3>

<p>CAS is an indivisible operation (<em>[lock] cmpxchg</em> instruction on x86) that compares an expected
  value at a memory address and replaces it with a desired value, on fail it returns the current 
  value. Using CAS you could implement things like an atomic increment and basically any other
  atomic arithmetic operation. </p>

<p>But it gains additional meaning when used on pointers. Using CAS on a pointer could also be 
  viewed as a rename with a check. Actually <em>rename</em> is the way you do atomic file modifications 
  in Linux (rename system call is atomic if on the same device). Also important is that it 
  establishes an equivalence relationship between the pointer and the data it points to. In the 
  section on ABA we'll see how it plays out. </p>

<p>CAS provides the lock-free level of guarantee.</p>

<h3>LL/SC (Load Linked/Store Conditional).</h3>

<p>LL/SC operates with the help of a <em>link register</em> (one per core). By doing a special read 
  (lwarx on PowerPC, ldrex on ARM) the link register is initialized. Any processing is then 
  allowed and finished with a conditional store (stwcx on PowerPC, strex on ARM). </p>

<p>LL/SC is orthogonal to CAS because it's not comparing anything. It is also more general and 
  robust. But it requires more work from the OS developers: on context  switch a dummy conditional 
  store is necessary to clear the reservation. This also could manifest itself under heavy 
  contention: LL/SC could theoretically become obstruction free (CAS is always lock-free because 
  at least one thread will always succeed).</p>

<p><strong>Now to the evils that hunt lock-free programming.</strong> Here is a picture of a stack and two
  threads doing work.</p>

<p><center><img src="images/The-ABA-Problem.png" alt="" title="" /></center></p>

<h3>Evil Number One: the ABA</h3>

<p>As we observed above, CAS on pointers ties together the pointer and the data it points to.
  ABA happens when our idea about this relationship no longer holds true: the state of shared 
  data has changed (broken invariant) and we are unable to detect that with the CAS (LL/LC is 
  immune to this problem) on the pointer, because that pointer might have been deleted and 
  reallocated again. The ABA results in lost data and invalid data structure.</p>

<p>ABA can be solved in a number of ways: GC, hazard pointers, versioned pointers. Another solution
  is to make the data immutable in a strict sense. I mean, not only the data itself is constant,
  but also the address it occupies is unique. This is the way a versioned pointer works: we add
  a version tag to the pointer and this has the effect of increasing the address space.</p>

<p>It really helps to think about lock-free operations in the following way: as soon as we touched 
  any piece of shared data - we initiated a transaction, and the transaction should behave
  as stated by the ACID rules. </p>

<h3>Evil Number Two: Lifetime Management</h3>

<p><center><img src="images/atomic-cat.png" alt="" title="" /></center></p>

<p>In the above image the cat is an object. The object lifetime problem in multithreaded programming 
  is fundamental because at any point in time shared data could be accessed by a thread. Even 
  using locks won't be of great service here. Again, GC, hazard pointers or using  shared_ptr 
  can help, but they all are not ideal. GC needs a stop-the-world pause, hazard pointers require
  checking all threads, reference counting is a performance hit and might break out of control.</p>

<p>Now it is time for me to introduce <strong>atomic_data</strong> that is a good compromise between having a 
  lock-free data structure and avoiding the ABA and lifetime issues.</p>

<h3><strong>atomic_data</strong>: A Multibyte General Purpose Lock-Free Data Structure</h3>

<p><strong>atomic_data</strong> is a variant of RCU (Read-Copy-Update). Actually, at first I didn't know that fact,
  but then, while exploring, I came upon this <a href="http://www.rdrop.com/~paulmck/RCU/">page</a> by 
  Paul McKenney. After studying different implementations I came to a conclusion that 
  <strong>atomic_data</strong> has a novel design and therefore has some value for the community. But if you 
  feel like that was already done before - feel free to leave a comment.</p>

<p>The one important aspect that divides RCU techniques is in how reclamation of used memory is
  done. We need a grace period - when data is no longer accessed by threads - to do cleaning.
  This way we solve the lifetime management problem but not necessarily the ABA problem. </p>

<p><strong>atomic_data</strong> is a template that wraps any data structure. There is a static preallocated 
  circular lock-free queue of this data of desired length. C++ rules for templates and static 
  data makes it one instance per data type. The following illustration will make it easier to 
  understand.</p>

<p><center><img src="images/atomic-data.png" alt="" title="" /></center></p>

<p>During operation the update and read methods are being called that accept a functor (a lambda)
  as a parameter provided by the user. It atomically allocates an element from the circular 
  lock-free queue, makes a copy of the current data and passes it to the functor. After the functor 
  finishes, the update method tries to do CAS on the pointer to current data. In case of failure 
  the steps are repeated. On success the now old data element is being atomically returned to the 
  circular queue. There is also an update_weak method that doesn't loop and returns a boolean.</p>

<p>So how do we avoid reusing used data and the ABA problem? That's where <strong>atomic_data</strong> differs:
  we introduce a synchronization barrier when the left pointer (atomic integer) modulo N
  (length of the queue) equals zero (modulo for N power of two is going to be faster).  The
  barrier separates used elements from unused. By waiting at the barrier for the condition
  <em>(right - left) == N</em> (where N is length of the queue) we make sure that no threads access
  the data elements from the queue in the update method. The used elements are now ready to be
  safely used again. This solves the lifetime and ABA problems at once. By increasing the length
  of the queue we are able to offset the cost of this synchronization.</p>

<p>But careful readers noticed that some elements in the queue might still be accessed by readers.
  There are a number of options to solve this. <strong>atomic_data</strong> uses a static atomic reader counter
  with relaxed memory order (language rules make it one per data type). Yes, this adds a point of 
  contention, but it's a good compromise because makes it easy to check for readers at the 
  synchronization barrier.</p>

<h3>Perfomance Cost</h3>

<p>The cost of <strong>atomic_data</strong>: an atomic increment and decrement for readers with relaxed memory 
  order, two CAS on a pointer and a data structure copy for writers (there is always one successful 
  thread), and on hitting the barrier we wait for all threads to stop working with data.</p>

<h3>Exceptions</h3>

<p><strong>atomic_data</strong> is tolerant to exceptions: it catches all exceptions, does cleanup and rethrows.</p>

<h3>Shortcomings of <strong>atomic_data</strong></h3>

<p>Although <strong>atomic_data</strong> is quite robust and lock-free, it is still a compromise. First, there is
  a contention point on the readers counter. Secondly, there is a sync barrier. Upon hitting this
  barrier we should wait for all threads - writers and readers - to finish their work.</p>

<p>Also it's not tolerant to a thread kill or suspension. But this seems far-fetched. We write
  our programs under the assumption that the hardware and OS will behave as documented. Threads 
  are no worse than processes. There should be no unexpected thread/process kill signals or 
  suspensions. It's a completely different story if a programmer does that on purpose.</p>

<p><strong>atomic_data::update</strong> is not reentrable for a thread: calling <strong>atomic_data::update</strong> from 
  inside <strong>atomic_data::update</strong> can cause an infinite wait on the barrier.</p>

<h3>How Long Should Be the Queue of Preallocated Data Elements?</h3>

<p>This question might seem to be trivial, but it leads to interesting results on the pros/cons
  of lock-free data structures. For update-only scenarios it doesn't make much sense to have a 
  lengthy queue because only one thread will ever succeed, all other threads will do useless 
  work. In fact there is a better concurrent data structure for such cases: messaging queues.
  A messaging queue allows one to monitor the load, also messages can be reordered for better
  efficiency. And it's relatively easy to implement.</p>

<p>For other cases it makes sense to make the queue length equal the number of threads (and power 
  of two for better efficiency). You should really measure the frequency of sync events to make a 
  good decision. For example, if reading isn't fast then it makes sense to bump up the queue size.</p>

<h3>Code Samples</h3>

<p><strong>atomic_data</strong> is easy to use and it works on any copyable data structure. Although you should 
  always keep in mind the cost of copying.</p>

<p>Let's implement an atomic lock-free array. The threads will scan the array and increment the
  minimal value:</p>

<pre><code>    template&lt;typename T, size_t N&gt;
    struct atomic_array {
        static const size_t size = N;
        T data[N];
    };

    using array_t = atomic_array&lt;int, 16&gt;;

    atomic_data&lt;array_t&gt; array0;

    //called by each thread
    void update_array() {
      array0.update( []( array_t* array_new ) {

        int min = 0;
        size_t min_index = 0;

        //look up minimum value
        for( size_t i = 0; i &lt; array_t.size; i++ ) {
            if( array_new-&gt;data[i] &lt; min ) {
              min = array_new-&gt;data[i];
              min_index = i;
            }
        }

        array_new-&gt;data[ min_index ]++;

      } );
    }
</code></pre>

<p>Remember, you can't touch any data that doesn't belong to <strong>atomic_data</strong>. So, for example, you 
  can't really make a double ended linked list, unless you make the size fixed and store the entire 
  list in the <strong>atomic_data</strong>.</p>

<h3>Lock-free std::map?</h3>

<p>Actually the design of the <strong>atomic_data</strong> makes it easy to make std::map (or anything else) 
  lock-free. </p>

<pre><code>    atomic_data&lt; std::map&lt;key,value&gt; &gt; atomic_map0;

    atomic_map0.update( []( auto *map_new ) {
        map_new-&gt;insert( { key, value } );
    } );

    atomic_map0.read( [&amp;value]( auto *map ) mutable {
        auto it = map-&gt;find( key );
        if( it != map-&gt;end() ) value = *it;
    } );
</code></pre>

<p>Technically it isn't lock-free because of memory allocation. To make it truly lock-free we
  need to implement a lock-free allocator and the design of <strong>atomic_data</strong> helps: defer memory
  freeing by storing a pointer in a data field and then, in a copy constructor, you check for it.
  Since data elements from the queue are allocated atomically to every thread, it's going to be 
  safe.</p>


<script src="highlight/highlight.pack.js"></script>
<script>
  //hljs.configure({languages: ["C++"]});
  hljs.initHighlightingOnLoad();
</script>


<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    if( ! document.getElementById("contents") ) // alexpolt: exclude main page with contents
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        
        s.src = '//alexpolt-github-io.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



  </div> <!-- end content -->

</div> <!-- end wrap -->



<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75341409-1', 'auto');
  ga('send', 'pageview');
</script>

</BODY>

